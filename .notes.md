# Test Cases 

### Smoke Test
The purpose for smoke test is validate that the key endpoints works under normal load.

**Setup:**
- VUs: 1–5
- Duration: 30–60s

**Endpoints tested:**
- POST /auth (create token)
- POST /booking (create booking)
- GET /booking/{id} (fetch booking)

**Questions:**
- Do requests return HTTP 200?
- Is response time < 500ms?

**Expected Result:**
- Success rate > 99%
- p95 latency < 500ms

### Load Test
The purpose for load test is to simulate production-like load.

**Setup:**
- VUs: 50–100
- Duration: 5–10 mins
-Endpoints: same as smoke, but heavier mix (read-heavy: 70% GET, 20% POST, 10% DELETE/PUT).

**Questions:**
- Can API sustain 100 VUs?
- Does throughput stay > 100 requests/s?
- Are error rates < 1%?

**Expected Result:**
- SLA thresholds met
- System does not degrade noticeably


### Stress Test
The purpose of stress test is to find an api breaking point.

**Setup:**
- Ramp-up load gradually: e.g., start 50 VUs → +50 every 2 min until failure or 500 VUs.

**Questions:**
- At what VU count does response time exceed 1s?
- When do error rates spike above 5%?

**Expected Result:**
- System handles load until limit is reached
- Breaking point identified (e.g., 300 VUs = 10% errors)

### Endurance Test
The purpose of endurance test is to check long-term duraility.

**Setup:**
- VUs: 20–30 (moderate)
- Duration: 8–24h
- Endpoints: All CRUD operations

**Questions:**
- Does response time degrade over time?
- Any memory leaks (heap growth, CPU usage)?
- Is availability stable?

**Expected Result:**
- Stable throughput
- Error rate < 1% for entire duration

### Perfomace Test
The purpose of perfomance testing is to validate SLA thresholds across all tests. 

**Setup:** Applied to all scenarios

**Metrics:**
- Response Time: p95 < 500ms
- Error Rate: < 1%
- Availability: 99.9% uptime
- Throughput: > 100 requests/s

**Questions:**
- Did all scenarios meet SLA thresholds?

**Expected Result:**
- ✅ if thresholds met, ❌ if not